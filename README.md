Income Estimation Model â€“ Preprocessing Steps ðŸ› ï¸

To prepare the data for a Random Forest model, I followed these steps:

1. Descriptive Stats ðŸ“Š  
   Took a quick look at the data â€” checked distributions, spotted weird stuff, and got a feel for what I was working with.

2. Feature Engineering ðŸ§   
   Built some new features using mean and max values:  
   - `payment_period` grouped by binned age âž¡ï¸ `payment_period_mean_by_bin_age`, `payment_period_max_by_bin_age`  
   - `amount` by `txn_description` âž¡ï¸ `amount_mean_by_txn_description`, `amount_max_by_txn_description`  
   - `balance` by `merchant_suburb` âž¡ï¸ `balance_mean_by_merchant_suburb`, `balance_max_by_merchant_suburb`

3. High Cardinality Cleanup ðŸ§¹
   Dropped categorical columns with 10+ unique values â€” too noisy, not worth it.

4. Handling Missing Values ðŸ•³ï¸âž¡ï¸ðŸ“¦  
   Filled missing data using the mode (most common value). Simple and effective!

5. Categorical to Numeric ðŸ”¬  
   Used `get_dummies()` to convert categorical features into numeric ones â€” ensuring compatibility with machine learning algorithms.

Modeling Steps ðŸ¤–


I built three versions of the model:

Default model â€“ just using the default parameters

Feature importanceâ€“based model â€“ built using all features, but paying attention to their importance

Optimized model â€“ built using only the most relevant features and tuned parameters

For each version, I evaluated the model using Train and Test RÂ² scores to compare performance ðŸ“ˆ

ðŸ‘‰ For the optimized model, I only included features with importance > 1%, and then fine-tuned the parameters to get the best performance.

Univariate Analysis ðŸ“Š


I performed univariate RÂ² analysis on the optimized model to see how much each feature contributes on its own.

âœ… I kept only the features where:

The Test RÂ² score was above 5%, and

The feature didnâ€™t cause overfitting (i.e., the gap between train and test RÂ² was reasonable)

This helped me focus on the most stable and meaningful predictors ðŸ’¡

Final Model ðŸ§©


For the final model, I used only the top-performing variables â€” the ones that had the biggest impact on model accuracy.

âœ… This helped simplify the model while keeping performance strong and stable.

The result: a cleaner, more focused model that does the job without unnecessary complexity ðŸš€
